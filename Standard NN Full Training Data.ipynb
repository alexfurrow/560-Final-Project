{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Standard Neural Network for Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Following the code here: https://github.com/ychennay/dso-560-nlp-and-text-analytics/blob/master/week7/Deep%20Learning%20with%20Word%20Embeddings.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "from typing import List\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Prepare to Iterate through All Tags\n",
    "#- The goal is to train a neural network to predict for each tag\n",
    "#- In order to do so, we must read in the CSVs created by the \"One Hot Encoding and Duplicate Removal\" python notebook\n",
    "\n",
    "fulldata = pd.read_csv(\"full_data_joined_attr_processed.csv\") #gathering data as created through the notebook that gathers and cleans the input variables\n",
    "testdata = pd.read_csv(\"full_data_to_predict.csv\")['final']\n",
    "testdata = list(testdata.astype('str'))\n",
    "\n",
    "chosen_cats = ['style','embellishment','occasion','category','dry_clean_only'] #gathering the attribute categories (5 for our group)\n",
    "\n",
    "#gather the unique tags for each of the 5 catgories\n",
    "styles = fulldata[fulldata['attribute_name'] == 'style']['attribute_value'].unique()\n",
    "embels = fulldata[fulldata['attribute_name'] == 'embellishment']['attribute_value'].unique()\n",
    "occasi = fulldata[fulldata['attribute_name'] == 'occasion']['attribute_value'].unique()\n",
    "catego = fulldata[fulldata['attribute_name'] == 'category']['attribute_value'].unique()\n",
    "drycle = fulldata[fulldata['attribute_name'] == 'dry_clean_only']['attribute_value'].unique()\n",
    "\n",
    "#conver to lists\n",
    "styles = list(styles)\n",
    "embels = list(embels)\n",
    "occasi = list(occasi)\n",
    "catego = list(catego)\n",
    "drycle = list(drycle)\n",
    "\n",
    "#combine tags to create a single list\n",
    "tags = styles + embels + occasi + catego + drycle\n",
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading in the documents associated with each tag\n",
    "#NOTE: \"One Hot Encoding and Duplicate Removal.ipybn\" must be run to create the CSVs used in this cell\n",
    "\n",
    "docs = [] #for the X values in our neural net\n",
    "labels = [] #for the Y values in our neural net\n",
    "for i in range(0,len(tags)):\n",
    "    docs.append(pd.read_csv(f\"{tags[i]}_NN.csv\")['final']) #reading in the documents of 1 hot encoding, final is the column of product documents\n",
    "    labels.append(array(pd.read_csv(f\"{tags[i]}_NN.csv\")[tags[i]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3916,\n",
       " 3916,\n",
       " 3916,\n",
       " 3916,\n",
       " 3916,\n",
       " 3916,\n",
       " 3916,\n",
       " 3916,\n",
       " 3916,\n",
       " 3916,\n",
       " 3916,\n",
       " 70,\n",
       " 70,\n",
       " 70,\n",
       " 70,\n",
       " 70,\n",
       " 70,\n",
       " 70,\n",
       " 70,\n",
       " 70,\n",
       " 3914,\n",
       " 3914,\n",
       " 3914,\n",
       " 3914,\n",
       " 3914,\n",
       " 3914,\n",
       " 3914,\n",
       " 3969,\n",
       " 3969,\n",
       " 3969,\n",
       " 3969,\n",
       " 3969,\n",
       " 3969,\n",
       " 3969,\n",
       " 3969,\n",
       " 2776,\n",
       " 2776]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We are training on the full data set because we are using this model to predict\n",
    "training_sets = []\n",
    "for i in range(0,len(tags)):\n",
    "    training_sets.append(int(len(docs[i]))) #full training set\n",
    "training_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We are training on the full data set, so Xtest_sets and Ytest_sets are commented out\n",
    "Xtrain_sets = []\n",
    "Ytrain_sets = []\n",
    "# Xtest_sets = []\n",
    "# Ytest_sets = []\n",
    "\n",
    "#Creating the training sets\n",
    "for i in range(0,len(tags)):\n",
    "    Xtrain_sets.append(docs[i].iloc[:training_sets[i]])\n",
    "    Ytrain_sets.append(labels[i][:training_sets[i]])\n",
    "#     Xtest_sets.append(list(docs[i].iloc[training_sets[i]:]))\n",
    "#     Ytest_sets.append(labels[i][training_sets[i]:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the Neural Nets Iteratively for Each Tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating some useful functions used in the next cell\n",
    "def integer_encode_documents(docs: List[str], tokenizer: Tokenizer)-> List[List[int]]:\n",
    "    documents = []\n",
    "    for d in docs:\n",
    "        doc_integers = []\n",
    "        for i in text_to_word_sequence(d):\n",
    "            doc_integers.append(tokenizer.word_index[i])\n",
    "        documents.append(doc_integers)\n",
    "    return documents\n",
    "\n",
    "def integer_encode_documents(docs, tokenizer):\n",
    "    return tokenizer.texts_to_sequences(docs)\n",
    "\n",
    "def get_max_token_length_per_doc(docs: List[List[str]])-> int:\n",
    "    return max(list(map(lambda x: len(x.split()), docs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\furro\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\furro\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\furro\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\furro\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\furro\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\furro\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\furro\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\furro\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\furro\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\furro\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\furro\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\furro\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\furro\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\furro\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\furro\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\furro\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\furro\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\furro\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\furro\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\furro\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\furro\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\furro\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\furro\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\furro\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\furro\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\furro\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\furro\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\furro\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\furro\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\furro\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\furro\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\furro\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\furro\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\furro\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\furro\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\furro\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\furro\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "#This cell trains and tests a neural network for each of the categories. \n",
    "EMBEDDING_SIZE = 12 #the default was 8 so we kept it small, although we saw embedding size of 100 in another example. 8 is good because it keeps dimensions low\n",
    "#set_of_training_accuracies = [] #for recording the training accuracy\n",
    "set_of_predictions = [] #for recording the predictions on the untagged data\n",
    "\n",
    "for i in range(0,len(tags)):\n",
    "    docs = Xtrain_sets[i] #gather one training set at a time\n",
    "    vocab_size = 10000 #make this high enough that no errors happen\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(docs)\n",
    "    \n",
    "    # integer encode the documents\n",
    "    encoded_docs = integer_encode_documents(docs, tokenizer)# this is a list of lists, the numbers represent the index position of that word.\n",
    "    max_length = get_max_token_length_per_doc(docs)    # get the max length in terms of token length\n",
    "    \n",
    "    # pad documents to a max length of 4 words\n",
    "    padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "    \n",
    "    model = Sequential() #Simple neural network\n",
    "    model.add(Embedding(vocab_size, EMBEDDING_SIZE, input_length=max_length))\n",
    "    model.add(Flatten()) #Flatten makes this a NN x 1 vector.\n",
    "    model.add(Dense(1, activation='sigmoid')) # these 32 elements are coalesced into one final output node, a sigmoid that outputs a probability of positive or negative\n",
    "\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc']) # compile the model\n",
    "\n",
    "    model.fit(padded_docs, Ytrain_sets[i], epochs=50, verbose=0) # fit the model\n",
    "\n",
    "    loss, accuracy = model.evaluate(padded_docs, Ytrain_sets[i], verbose=0)\n",
    "\n",
    "    embedding_layer = model.layers[0]\n",
    "    embedding_layer.get_weights()[0].shape\n",
    "    \n",
    "    encoded_test_docs = integer_encode_documents(testdata, tokenizer)\n",
    "\n",
    "    padded_test_docs = pad_sequences(encoded_test_docs, maxlen=max_length, padding='post')# pad test documents\n",
    "    prediction = model.predict(padded_test_docs, verbose=0)\n",
    "    prediction[prediction>0.5] = 1\n",
    "    prediction[prediction<=0.5] = 0\n",
    "    prediction = prediction.flatten()\n",
    "    set_of_predictions.append(prediction)\n",
    "#     correct = (prediction == Ytest_sets[i]) #this and the following were used in training/validation stage, commmented out here\n",
    "#     total = len(correct)\n",
    "#     true = np.count_nonzero(correct == True)\n",
    "#     test_accuracy = true/total\n",
    "#     set_of_test_accuracies.append(test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the output with predictions on all untagged products (i.e. the test data set)\n",
    "\n",
    "output = pd.DataFrame()\n",
    "output['product_id'] = pd.read_csv(\"full_data_to_predict.csv\")['product_id']\n",
    "for i in range(0,len(tags)):\n",
    "    output[i+1] = set_of_predictions[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags.insert(0,'product_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "output.columns = tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>classic</th>\n",
       "      <th>modern</th>\n",
       "      <th>casual</th>\n",
       "      <th>romantic</th>\n",
       "      <th>glam</th>\n",
       "      <th>businesscasual</th>\n",
       "      <th>edgy</th>\n",
       "      <th>retro</th>\n",
       "      <th>androgynous</th>\n",
       "      <th>...</th>\n",
       "      <th>top</th>\n",
       "      <th>onepiece</th>\n",
       "      <th>bottom</th>\n",
       "      <th>shoe</th>\n",
       "      <th>sweater</th>\n",
       "      <th>accessory</th>\n",
       "      <th>blazerscoatsjackets</th>\n",
       "      <th>sweatshirthoodie</th>\n",
       "      <th>yes</th>\n",
       "      <th>no</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>01DSRPSZTDW2PGK1YWYXJGKZZ0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>01DSQXJBX0R7DCW7KTAC1SW547</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>01DPGV8TGRAB993PF7Z3YWG2VR</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>01DSR8G3F7DBRTMP8THF97XSQ2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>01DSR8G5GP519DEDCSKBMWQVK5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44135</td>\n",
       "      <td>01E2P33K4RRKYDCGN6WX8X1HCJ</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44136</td>\n",
       "      <td>01E2P30V4VR23GS7ZT9CTQZBCG</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44137</td>\n",
       "      <td>01E2P4J0GC34Z496D78AMNV0WC</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44138</td>\n",
       "      <td>01E5ZW1B7Q3AFDD1RTXC4RTZZN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44139</td>\n",
       "      <td>01E5ZA21KKNY5RF7RH7VCS53SS</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>44140 rows Ã— 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       product_id  classic  modern  casual  romantic  glam  \\\n",
       "0      01DSRPSZTDW2PGK1YWYXJGKZZ0      0.0     1.0     1.0       0.0   0.0   \n",
       "1      01DSQXJBX0R7DCW7KTAC1SW547      0.0     0.0     1.0       0.0   0.0   \n",
       "2      01DPGV8TGRAB993PF7Z3YWG2VR      0.0     1.0     1.0       0.0   0.0   \n",
       "3      01DSR8G3F7DBRTMP8THF97XSQ2      0.0     0.0     1.0       0.0   0.0   \n",
       "4      01DSR8G5GP519DEDCSKBMWQVK5      1.0     0.0     1.0       0.0   0.0   \n",
       "...                           ...      ...     ...     ...       ...   ...   \n",
       "44135  01E2P33K4RRKYDCGN6WX8X1HCJ      1.0     0.0     1.0       0.0   0.0   \n",
       "44136  01E2P30V4VR23GS7ZT9CTQZBCG      0.0     1.0     1.0       0.0   0.0   \n",
       "44137  01E2P4J0GC34Z496D78AMNV0WC      1.0     1.0     1.0       0.0   0.0   \n",
       "44138  01E5ZW1B7Q3AFDD1RTXC4RTZZN      1.0     0.0     0.0       0.0   1.0   \n",
       "44139  01E5ZA21KKNY5RF7RH7VCS53SS      0.0     0.0     1.0       0.0   0.0   \n",
       "\n",
       "       businesscasual  edgy  retro  androgynous  ...  top  onepiece  bottom  \\\n",
       "0                 0.0   0.0    0.0          0.0  ...  0.0       0.0     0.0   \n",
       "1                 0.0   0.0    0.0          0.0  ...  0.0       0.0     0.0   \n",
       "2                 0.0   1.0    0.0          0.0  ...  0.0       0.0     0.0   \n",
       "3                 0.0   0.0    0.0          0.0  ...  0.0       0.0     0.0   \n",
       "4                 0.0   0.0    0.0          0.0  ...  0.0       0.0     0.0   \n",
       "...               ...   ...    ...          ...  ...  ...       ...     ...   \n",
       "44135             0.0   0.0    0.0          0.0  ...  0.0       0.0     0.0   \n",
       "44136             0.0   1.0    0.0          0.0  ...  0.0       0.0     0.0   \n",
       "44137             0.0   0.0    0.0          0.0  ...  0.0       0.0     0.0   \n",
       "44138             1.0   0.0    0.0          0.0  ...  0.0       0.0     0.0   \n",
       "44139             1.0   0.0    0.0          0.0  ...  1.0       0.0     0.0   \n",
       "\n",
       "       shoe  sweater  accessory  blazerscoatsjackets  sweatshirthoodie  yes  \\\n",
       "0       1.0      0.0        0.0                  0.0               0.0  0.0   \n",
       "1       0.0      0.0        0.0                  0.0               0.0  0.0   \n",
       "2       0.0      0.0        1.0                  0.0               0.0  0.0   \n",
       "3       0.0      0.0        0.0                  0.0               0.0  0.0   \n",
       "4       0.0      0.0        0.0                  0.0               0.0  0.0   \n",
       "...     ...      ...        ...                  ...               ...  ...   \n",
       "44135   1.0      0.0        0.0                  0.0               0.0  0.0   \n",
       "44136   1.0      0.0        0.0                  0.0               0.0  0.0   \n",
       "44137   1.0      0.0        0.0                  0.0               0.0  0.0   \n",
       "44138   1.0      0.0        0.0                  0.0               0.0  0.0   \n",
       "44139   0.0      0.0        0.0                  0.0               0.0  0.0   \n",
       "\n",
       "        no  \n",
       "0      1.0  \n",
       "1      1.0  \n",
       "2      1.0  \n",
       "3      1.0  \n",
       "4      1.0  \n",
       "...    ...  \n",
       "44135  1.0  \n",
       "44136  1.0  \n",
       "44137  1.0  \n",
       "44138  1.0  \n",
       "44139  1.0  \n",
       "\n",
       "[44140 rows x 38 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important Note/Weakness of This Output\n",
    "\n",
    "The model structure predicted a 1/0 (yes/no) on each of the tags individually. This is a good strategy for categories that follow a \"Select All that Apply\" criteria (e.g. a product can be classified as multiple styles: athleisure, casual, etc).\n",
    "\n",
    "But for categories that follow a \"Choose 1\" criteria (Category and Dry Clean Only) - this method fails. See the cell below this one for a demonstration.\n",
    "\n",
    "Our group knows that a multiple classification neural network, with a softmax activation function and a categorical crossentropy loss function would be the correct start to resolving this issue. Frankly, we did not have time to resolve this issue in the proper manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(output['top']+output['bottom']>=2) #This sum should be 0, as a product should only be classified as either \"top\" or \"bottom\" and not both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merging the original file with the predictions on product_id\n",
    "x = pd.read_csv(\"full_data_to_predict.csv\") #reading in original file\n",
    "output2 = x.merge(output,on=\"product_id\") #mergin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping unnecessary columns from the original file\n",
    "output3 = output2.drop(['Unnamed: 0', 'dry_clean_only','category',  'combined_data', 'rm_sw', 'lemmatized', 'final_list', 'studs_x', 'sequins_x',\n",
    "       'embroidery_x', 'trim_x', 'ruffles_x', 'mesh_x', 'lace_x', 'fringe_x',\n",
    "       'buckles_x','crystals', 'rhinestone', 'patches', 'epaulets', 'beaded',\n",
    "       'modern_x', 'romantic_x', 'classic_x', 'casual_x', 'businesscasual_x',\n",
    "       'glam_x', 'edgy_x', 'retro_x', 'androgynous_x', 'boho_x',\n",
    "       'athleisure_x'], axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "output3.to_csv(\"output.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
