{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Standard Neural Network for Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Following the code here: https://github.com/ychennay/dso-560-nlp-and-text-analytics/blob/master/week7/Deep%20Learning%20with%20Word%20Embeddings.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "from typing import List\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Prepare to Iterate through All Tags\n",
    "#- The goal is to train a neural network to predict for each tag\n",
    "#- In order to do so, we must read in the CSVs created by the \"One Hot Encoding and Duplicate Removal\" python notebook\n",
    "\n",
    "fulldata = pd.read_csv(\"full_data_joined_attr_processed.csv\") #gathering data as created through the notebook that gathers and cleans the input variables\n",
    "\n",
    "chosen_cats = ['style','embellishment','occasion','category','dry_clean_only'] #gathering the attribute categories (5 for our group)\n",
    "\n",
    "#gather the unique tags for each of the 5 catgories\n",
    "styles = fulldata[fulldata['attribute_name'] == 'style']['attribute_value'].unique()\n",
    "embels = fulldata[fulldata['attribute_name'] == 'embellishment']['attribute_value'].unique()\n",
    "occasi = fulldata[fulldata['attribute_name'] == 'occasion']['attribute_value'].unique()\n",
    "catego = fulldata[fulldata['attribute_name'] == 'category']['attribute_value'].unique()\n",
    "drycle = fulldata[fulldata['attribute_name'] == 'dry_clean_only']['attribute_value'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "styles = list(styles)\n",
    "embels = list(embels)\n",
    "occasi = list(occasi)\n",
    "catego = list(catego)\n",
    "drycle = list(drycle)\n",
    "\n",
    "tags = styles + embels + occasi + catego + drycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading in the documents associated with each tag\n",
    "docs = [] #for the X values in our neural net\n",
    "labels = [] #for the Y values in our neural net\n",
    "for i in range(0,len(tags)):\n",
    "    docs.append(pd.read_csv(f\"{tags[i]}_NN.csv\")['final']) #reading in the documents of 1 hot encoding, final is the column of product documents\n",
    "     #for example: pd.read_csv(\"classic_NN.csv\")['final'] #would get the documents for all products that have style\n",
    "    labels.append(array(pd.read_csv(f\"{tags[i]}_NN.csv\")[tags[i]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3132,\n",
       " 3132,\n",
       " 3132,\n",
       " 3132,\n",
       " 3132,\n",
       " 3132,\n",
       " 3132,\n",
       " 3132,\n",
       " 3132,\n",
       " 3132,\n",
       " 3132,\n",
       " 56,\n",
       " 56,\n",
       " 56,\n",
       " 56,\n",
       " 56,\n",
       " 56,\n",
       " 56,\n",
       " 56,\n",
       " 56,\n",
       " 3131,\n",
       " 3131,\n",
       " 3131,\n",
       " 3131,\n",
       " 3131,\n",
       " 3131,\n",
       " 3131,\n",
       " 3175,\n",
       " 3175,\n",
       " 3175,\n",
       " 3175,\n",
       " 3175,\n",
       " 3175,\n",
       " 3175,\n",
       " 3175,\n",
       " 2220,\n",
       " 2220]"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Each neural net will have a different number of documents to train and test on, here we create a list of \n",
    "training_sets = []\n",
    "for i in range(0,len(tags)):\n",
    "    training_sets.append(int(len(docs[i])*.8)) #80%-20% train-test split\n",
    "training_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray, pandas.core.series.Series)"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(labels[0]), type(docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain_sets = []\n",
    "Ytrain_sets = []\n",
    "Xtest_sets = []\n",
    "Ytest_sets = []\n",
    "\n",
    "for i in range(0,len(tags)):\n",
    "    Xtrain_sets.append(docs[i].iloc[:training_sets[i]])\n",
    "    Ytrain_sets.append(labels[i][:training_sets[i]])\n",
    "    Xtest_sets.append(list(docs[i].iloc[training_sets[i]:]))\n",
    "    Ytest_sets.append(labels[i][training_sets[i]:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the Neural Nets Iteratively for Each Tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating some useful functions\n",
    "def integer_encode_documents(docs: List[str], tokenizer: Tokenizer)-> List[List[int]]:\n",
    "    documents = []\n",
    "    for d in docs:\n",
    "        doc_integers = []\n",
    "        for i in text_to_word_sequence(d):\n",
    "            doc_integers.append(tokenizer.word_index[i])\n",
    "        documents.append(doc_integers)\n",
    "    return documents\n",
    "\n",
    "def integer_encode_documents(docs, tokenizer):\n",
    "    return tokenizer.texts_to_sequences(docs)\n",
    "\n",
    "def get_max_token_length_per_doc(docs: List[List[str]])-> int:\n",
    "    return max(list(map(lambda x: len(x.split()), docs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\furro\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\furro\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\furro\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\furro\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\furro\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\furro\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\furro\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\furro\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\furro\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\furro\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\furro\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\furro\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\furro\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\furro\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\furro\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\furro\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\furro\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\furro\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\furro\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\furro\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\furro\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\furro\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\furro\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\furro\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\furro\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\furro\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\furro\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\furro\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\furro\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\furro\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\furro\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\furro\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\furro\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\furro\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\furro\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\furro\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\furro\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "#The following cell trains and tests a neural network for each of the categories. \n",
    "EMBEDDING_SIZE = 12\n",
    "set_of_training_accuracies = []\n",
    "set_of_test_accuracies = []\n",
    "for i in range(0,len(tags)):\n",
    "    docs = Xtrain_sets[i] #for ease of re-using code\n",
    "    vocab_size = 5400\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(docs)\n",
    "    # integer encode the documents\n",
    "    encoded_docs = integer_encode_documents(docs, tokenizer)# this is a list of lists, the numbers represent the index position of that word.\n",
    "    max_length = get_max_token_length_per_doc(docs)    # get the max length in terms of token length\n",
    "    # pad documents to a max length of 4 words\n",
    "    padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, EMBEDDING_SIZE, input_length=max_length))\n",
    "    model.add(Flatten()) #Flatten makes this a NN x 1 vector.\n",
    "    model.add(Dense(1, activation='sigmoid')) # these 32 elements are coalesced into one final output node, a sigmoid that outputs a probability of positive or negative\n",
    "\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc']) # compile the model\n",
    "\n",
    "    model.fit(padded_docs, Ytrain_sets[i], epochs=50, verbose=0) # fit the model\n",
    "\n",
    "    loss, accuracy = model.evaluate(padded_docs, Ytrain_sets[i], verbose=0)\n",
    "    set_of_training_accuracies.append(f'{tags[i]} Accuracy: %f' % (accuracy*100)) \n",
    "\n",
    "    embedding_layer = model.layers[0]\n",
    "    embedding_layer.get_weights()[0].shape\n",
    "    \n",
    "    encoded_test_docs = integer_encode_documents(Xtest_sets[i], tokenizer)\n",
    "\n",
    "    padded_test_docs = pad_sequences(encoded_test_docs, maxlen=max_length, padding='post')# pad test documents\n",
    "    prediction = model.predict(padded_test_docs, verbose=0)\n",
    "    prediction[prediction>0.5] = 1\n",
    "    prediction[prediction<=0.5] = 0\n",
    "    prediction = prediction.flatten()\n",
    "    correct = (prediction == Ytest_sets[i])\n",
    "    total = len(correct)\n",
    "    true = np.count_nonzero(correct == True)\n",
    "    test_accuracy = true/total\n",
    "    set_of_test_accuracies.append(test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 5400\n",
    "def make_lstm_classification_model(plot=False):\n",
    "    docs = Xtrain_sets[i] #for ease of re-using code\n",
    "    vocab_size = 5400\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(docs)\n",
    "    # integer encode the documents\n",
    "    encoded_docs = integer_encode_documents(docs, tokenizer)# this is a list of lists, the numbers represent the index position of that word.\n",
    "    max_length = get_max_token_length_per_doc(docs)    # get the max length in terms of token length\n",
    "    # pad documents to a max length of 4 words\n",
    "    padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(VOCAB_SIZE, 100, weights=[embedding_matrix], input_length=MAX_SEQUENCE_LENGTH, trainable=False))\n",
    "    model.add(Masking(mask_value=0.0)) # masking layer, masks any words that don't have an embedding as 0s.\n",
    "    model.add(LSTM(units=32, input_shape=(1, MAX_SEQUENCE_LENGTH)))\n",
    "    model.add(Dense(16))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc']) # compile the model\n",
    "\n",
    "    model.fit(padded_docs, , epochs=50, verbose=0) # fit the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_SIZE = 12\n",
    "set_of_training_accuracies = []\n",
    "set_of_test_accuracies = []\n",
    "for i in range(0,len(tags)):\n",
    "    docs = Xtrain_sets[i] #for ease of re-using code\n",
    "    vocab_size = 5400\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(docs)\n",
    "    # integer encode the documents\n",
    "    encoded_docs = integer_encode_documents(docs, tokenizer)# this is a list of lists, the numbers represent the index position of that word.\n",
    "    max_length = get_max_token_length_per_doc(docs)    # get the max length in terms of token length\n",
    "    # pad documents to a max length of 4 words\n",
    "    padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, EMBEDDING_SIZE, input_length=max_length))\n",
    "    model.add(Flatten()) #Flatten makes this a NN x 1 vector.\n",
    "    model.add(Dense(1, activation='sigmoid')) # these 32 elements are coalesced into one final output node, a sigmoid that outputs a probability of positive or negative\n",
    "\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc']) # compile the model\n",
    "\n",
    "    model.fit(padded_docs, Ytrain_sets[i], epochs=50, verbose=0) # fit the model\n",
    "\n",
    "    loss, accuracy = model.evaluate(padded_docs, Ytrain_sets[i], verbose=0)\n",
    "    set_of_training_accuracies.append(f'{tags[i]} Accuracy: %f' % (accuracy*100)) \n",
    "\n",
    "    embedding_layer = model.layers[0]\n",
    "    embedding_layer.get_weights()[0].shape\n",
    "    \n",
    "    encoded_test_docs = integer_encode_documents(Xtest_sets[i], tokenizer)\n",
    "\n",
    "    padded_test_docs = pad_sequences(encoded_test_docs, maxlen=max_length, padding='post')# pad test documents\n",
    "    prediction = model.predict(padded_test_docs, verbose=0) \n",
    "    prediction[prediction>0.5] = 1 #for the given tag, classify the product according to it's probability, 1 if p>0.5 \n",
    "    prediction[prediction<=0.5] = 0 #same but for 0\n",
    "    prediction = prediction.flatten() #flatten the array for the next step\n",
    "    correct = (prediction == Ytest_sets[i]) #for each tag, create an array that has True/False where True is a correctly predicted tag\n",
    "    total = len(correct) #find the total length of the array for computing the accuracy\n",
    "    true = np.count_nonzero(correct == True) #find the count of True values \n",
    "    test_accuracy = true/total #calculate the accuracy\n",
    "    set_of_test_accuracies.append(test_accuracy) #append the tag accuracy to the list of accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classic test accuracy is                0.5994897959183674\n",
      "modern test accuracy is                0.5816326530612245\n",
      "casual test accuracy is                0.7206632653061225\n",
      "romantic test accuracy is                0.7920918367346939\n",
      "glam test accuracy is                0.8150510204081632\n",
      "businesscasual test accuracy is                0.6479591836734694\n",
      "edgy test accuracy is                0.8252551020408163\n",
      "retro test accuracy is                0.9017857142857143\n",
      "androgynous test accuracy is                0.6135204081632653\n",
      "boho test accuracy is                0.8635204081632653\n",
      "athleisure test accuracy is                0.923469387755102\n",
      "studs test accuracy is                0.9285714285714286\n",
      "embroidery test accuracy is                0.7857142857142857\n",
      "trim test accuracy is                0.8571428571428571\n",
      "ruffles test accuracy is                0.7142857142857143\n",
      "mesh test accuracy is                0.8571428571428571\n",
      "fringe test accuracy is                1.0\n",
      "lace test accuracy is                1.0\n",
      "buckles test accuracy is                0.7857142857142857\n",
      "sequins test accuracy is                1.0\n",
      "daytonight test accuracy is                0.7100893997445722\n",
      "nightout test accuracy is                0.7203065134099617\n",
      "work test accuracy is                0.6538952745849298\n",
      "weekend test accuracy is                0.7611749680715197\n",
      "vacation test accuracy is                0.8301404853128991\n",
      "workout test accuracy is                0.9553001277139208\n",
      "coldweather test accuracy is                0.9808429118773946\n",
      "top test accuracy is                0.964735516372796\n",
      "onepiece test accuracy is                0.9861460957178841\n",
      "bottom test accuracy is                0.9886649874055415\n",
      "shoe test accuracy is                0.9962216624685138\n",
      "sweater test accuracy is                0.9861460957178841\n",
      "accessory test accuracy is                0.9798488664987406\n",
      "blazerscoatsjackets test accuracy is                0.9836272040302267\n",
      "sweatshirthoodie test accuracy is                0.9899244332493703\n",
      "yes test accuracy is                0.7607913669064749\n",
      "no test accuracy is                0.75\n"
     ]
    }
   ],
   "source": [
    "#Viewing test accuracy for comparison to other models\n",
    "for i in range(0,len(tags)):\n",
    "    print(f'{tags[i]} test accuracy is {\"\":>15}{set_of_test_accuracies[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8435369219774127"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(set_of_test_accuracies)/len(set_of_test_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
